---
title: "beyond univariate ddiff"
bibliography: 
  - roxygen.bib
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{beyond univariate ddiff}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Multivariate information diff
For each variables in corresponding datasets, we could treat them as independent variables or we could view them as correlated variables where they are generated under some multivariate distribution.  The `get_varialbe_multi_stat` is designed for obtaining custom difference metrics on these variables. 

First, we need to load the package and generate the simulated datasets : 
```{r results='markup'}
library(ddiff)
d <- generate_test_data(50)
```
For continuous variables, the default metric is set to be the variance-covariance matrix for multivariate case. The default metric for categorical and binary variables are contingency table and confusion matrix. 
```{r results='markup'}
#Multivariate information diff
ddiff_rpt <- get_variable_multi_stat(d$records_added$old, d$records_added$new,"id")
ddiff_rpt
```
For lower dimensional data, we could compare the variance-covariance directly while for high dimensional data, this approach would be computationally expensive. One possible way around is that, we could apply dimension reduction methods on this case. For example, we could look at the difference by projecting the data into two dimension plane and compare them directly. 
```{r results='markup'}
pca_multi_stat(d$records_added$old, d$records_added$new,"id", ddiff_rpt)
```

##  Overview 

This is a brief introduction to the package <tt>`fdapace`</tt> [@Gajardo21]. The basic work-flow behind the PACE approach for sparse functional data is as follows (see e.g. [@Yao05; @Liu09] for more information):

1. Calculate the smoothed mean $\hat{\mu}$ (using local linear smoothing) aggregating all the available readings together.
2. Calculate for each curve separately its own raw covariance and then aggregate all these raw covariances to generate the sample raw covariance.
3. Use the off-diagonal elements of the sample raw covariance to estimate the smooth covariance.
4. Perform eigenanalysis on the smoothed covariance to obtain the estimated eigenfunctions $\hat{\phi}$ and eigenvalues $\hat{\lambda}$, then project that smoothed covariance on a positive semi-definite surface [@Hall2008].
5. Use Conditional Expectation (PACE step) to estimate the corresponding scores $\hat{\xi}$.
ie. \newline 
$\hat{\xi}_{ik} = \hat{E}[\hat{\xi}_{ik}|Y_i] = \hat{\lambda}_k \hat{\phi}_{ik}^T \Sigma_{Y_i}^{-1}(Y_i-\hat{\mu}_i)$.

As a working assumption a dataset is treated as sparse if it has on average less than 20, potentially irregularly sampled, measurements per subject. A user can manually change the automatically determined <tt>`dataType`</tt> if that is necessary.
For densely observed functional data simplified procedures are available to obtain the eigencomponents and associated functional principal components scores (see eg.  [@Castro86] for more information). In particular in this case we:
 
1. Calculate the cross-sectional mean $\hat{\mu}$.
2. Calculate the cross-sectional covariance surface (which is guaranteed to be positive semi-definite).
3. Perform eigenanalysis on the covariance to estimate the eigenfunctions $\hat{\phi}$ and eigenvalues $\hat{\lambda}$.
4. Use numerical integration to estimate the corresponding scores $\hat{\xi}$.
  ie. \newline
 $\hat{\xi}_{ik} =  \int_0^T [ y(t) - \hat{\mu}(t)] \phi_i(t) dt$ 

In the case of sparse FPCA the most computational intensive part is the smoothing of the sample's raw covariance function. For this, we employ a local weighted bilinear smoother.


# Generating Data
We use the longitudinal dataset that this available with ddiff to showcase FPCA and its related functionality. The dataset containing [...]. The data are rather noisy, dense and with a characteristic flat start. For that reason in contrast with above we will use a smoothing estimating procedure despite having dense data.
```{r results='markup'}
fdata <- generate_test_functional_data()
```

# Apply functional PCA

```{r results='markup'}
Flies <- fdapace::MakeFPCAInputs(IDs = fdata$subjid, tVec = fdata$t_obs_n, yVec = fdata$y)
fpcaObjFlies <- fdapace::FPCA(Flies$Ly, Flies$Lt, list(methodMuCovEst = 'smooth', dataType='Dense', error = FALSE))
```
Based on the scree-plot we see that the first three components appear to encapsulate most of the relevant variation. The number of eigencomponents to reach a 99.99% FVE is 11 but just 6 eigencomponents are enough to reach a 95.0%. 
# FPCA in R using fdapace

```{r results='markup'}
par(mar=c(2.5,2.5,2.5,2.5))
plot(fpcaObjFlies)
```
# outlier detection
One can perform outlier detection (Febrero, Galeano, and GonzÃ¡lez-Manteiga 2007). Different ranking methodologies (KDE, bagplot Hyndman and Shang (2010) or point-wise) are available and can potentially identify different aspects of a sample. For example here it is notable that the kernel density estimator KDE variant identifies two main clusters within the main body of sample. 
```{r results='markup'}
fObj <- fpcaObjFlies
fScores1 <- fObj$xiEst[, 1]
fScores2 <- fObj$xiEst[, 2]
fScoresAll <- cbind(fScores1, fScores2)

fhat <- ks::kde(x = fScoresAll, gridsize = c(400, 400), compute.cont = TRUE)
zin = fhat$estimate
quickNNeval <- function(xin,yin, zin, xout, yout){
  xIndices = sapply( xout, function(myArg) which.min( abs( xin - myArg) ), simplify = TRUE)
  yIndices = sapply( yout, function(myArg) which.min( abs( yin - myArg) ), simplify = TRUE)
  return( zin[ cbind(xIndices,yIndices)] )
}
qq = quickNNeval(xin = fhat$eval.points[[1]], yin = fhat$eval.points[[2]], zin = zin, xout = fScores1, yout = fScores2)
xedge = 1.05 * max(abs(fScores1))
yedge = 1.05 * max(abs(fScores2))
args2 = list(x = fhat$eval.points[[1]], y = fhat$eval.points[[2]], 
               z = zin, labcex = 1.66, col = c("black", "blue","red"), levels = fhat$cont[c(50, 95, 99)], 
               labels = c("50%", "95%", "99%"))
args1 <- list(pch = 10, xlab = paste("FPC", 1, 
                                     " scores ", round(100 * fObj$cumFVE[1]), 
                                     "%", sep = ""), ylab = paste("FPC", 2, 
                                                                  " scores ", round(diff(100 * fObj$cumFVE[c(2 - 
                                                                                                               1, 2)])), "%", sep = ""), xlim = c(-xedge, xedge), ylim = c(-yedge, yedge), lwd = 2)
do.call(graphics::contour, c(args2, args1))
  
grid(col = "#e6e6e6")
points(fScoresAll[qq <= fhat$cont[99], ], cex = 0.5, 
         col = "orange", pch = 10, lwd = 2)
points(fScoresAll[qq > fhat$cont[99] & qq <= fhat$cont[95], 
  ], cex = 0.33, col = "red", pch = 10, lwd = 2)
points(fScoresAll[qq > fhat$cont[95] & qq <= fhat$cont[50], 
  ], cex = 0.33, col = "blue", pch = 10, lwd = 2)
points(fScoresAll[qq >= fhat$cont[50], ], cex = 0.33, 
         col = "black", pch = 10, lwd = 2)
legend("bottomleft", c("< 50%", "50%-95%", "95%-99%", 
                         "> 99%"), pch = 19, col = c("black", "blue", 
                                                     "red", "orange"), pt.cex = 1.5, bg = "white")
title(main = "outlier detection for functional PCA with KDE of first two PC scores")
```
# diff version

We could also project the new data into the old space of scores. 
```{r results='markup'}
fdata_new <- fdata[sample(1:1000),]
fdata_new <- fdata_new %>% arrange(t_obs_n)
Flies <- fdapace::MakeFPCAInputs(IDs = fdata_new$subjid, tVec = fdata_new$t_obs_n, yVec = fdata_new$y)
length(unique(fdata_new$subjid))
pred <- predict(fObj, Flies$Ly, Flies$Lt, K=2)
pred$scores
```
```{r results='markup'}
do.call(graphics::contour, c(args2, args1))
  
grid(col = "#e6e6e6")
points(fScoresAll[qq <= fhat$cont[99], ], cex = 0.5, 
         col = "orange", pch = 10, lwd = 2)
points(fScoresAll[qq > fhat$cont[99] & qq <= fhat$cont[95], 
  ], cex = 0.33, col = "red", pch = 10, lwd = 2)
points(fScoresAll[qq > fhat$cont[95] & qq <= fhat$cont[50], 
  ], cex = 0.33, col = "blue", pch = 10, lwd = 2)
points(fScoresAll[qq >= fhat$cont[50], ], cex = 0.33, 
         col = "black", pch = 10, lwd = 2)
points(pred$scores, cex = 0.33, col = "green", pch = 10, lwd = 2)
legend("bottomleft", c("< 50%", "50%-95%", "95%-99%", 
                         "> 99%", "new"), pch = 19, col = c("black", "blue", 
                                                     "red", "orange","green"), pt.cex = 1.5, bg = "white")
title(main = "outlier detection for functional PCA with KDE of first two PC scores")
```
